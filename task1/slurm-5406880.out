/var/spool/slurm/slurmd/job5406880/slurm_script: line 2: SBATCH: command not found
/var/spool/slurm/slurmd/job5406880/slurm_script: line 3: SBATCH: command not found
/var/spool/slurm/slurmd/job5406880/slurm_script: line 5: SBATCH: command not found
/var/spool/slurm/slurmd/job5406880/slurm_script: line 6: SBATCH: command not found
/var/spool/slurm/slurmd/job5406880/slurm_script: line 7: SBATCH: command not found
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 160
slots that were requested by the application:

  ./balance

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------

scontrol show jobid 5406880
JobId=5406880 JobName=task1.sh
   UserId=lcl_uotiscsccs1044(1911044) GroupId=lcl_uotiscscc(1910004) MCS_label=N/A
   Priority=447338 Nice=0 Account=lcl-uotiscsccs QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   RunTime=00:00:03 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2021-05-25T09:49:49 EligibleTime=2021-05-25T09:49:49
   AccrueTime=2021-05-25T09:49:49
   StartTime=2021-05-25T11:19:34 EndTime=2021-05-25T11:19:37 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-05-25T11:19:34
   Partition=compute AllocNode:Sid=nia-login01:259343
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=nia2014
   BatchHost=nia2014
   NumNodes=1 NumCPUs=80 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=80,mem=175000M,node=1,billing=40
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryNode=175000M MinTmpDiskNode=0
   Features=[skylake|cascade] DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/gpfs/fs0/scratch/l/lcl_uotiscscc/lcl_uotiscsccs1044/zsr/task1/task1.sh
   WorkDir=/gpfs/fs0/scratch/l/lcl_uotiscscc/lcl_uotiscsccs1044/zsr/task1
   StdErr=/gpfs/fs0/scratch/l/lcl_uotiscscc/lcl_uotiscsccs1044/zsr/task1/slurm-5406880.out
   StdIn=/dev/null
   StdOut=/gpfs/fs0/scratch/l/lcl_uotiscscc/lcl_uotiscsccs1044/zsr/task1/slurm-5406880.out
   Power=
   MailUser=lcl_uotiscsccs1044@scinet.local MailType=NONE

sacct -j 5406880
       JobID    JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
5406880        task1.sh lcl-uotis+   00:00:03                        00:00.217  00:00.276      1:0 
5406880.bat+      batch lcl-uotis+   00:00:03    138528K      1248K  00:00.217  00:00.275      1:0 
5406880.ext+     extern lcl-uotis+   00:00:03    138360K       880K   00:00:00   00:00:00      0:0 
